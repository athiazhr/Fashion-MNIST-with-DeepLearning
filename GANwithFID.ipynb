{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generative Adversarial Network (GAN) adalah jenis arsitektur deep learning yang digunakan untuk menghasilkan data baru yang menyerupai data asli. GAN terdiri dari dua model neural network yang saling berlawanan, yaitu Generator dan Discriminator. Berikut adalah penjelasan cara kerja dari arsitektur GAN berdasarkan gambar yang disertakan:\n",
    "Komponen GAN\n",
    "1.\tGenerator: Model yang bertugas menghasilkan data baru yang menyerupai data asli. Generator mengambil input berupa noise acak (random noise) dan mengubahnya menjadi data yang mirip dengan data pelatihan asli.\n",
    "2.\tDiscriminator: Model yang bertugas membedakan antara data asli (dari data pelatihan) dan data palsu (dihasilkan oleh Generator). Discriminator mengeluarkan probabilitas apakah suatu data adalah asli atau palsu.\n",
    "Proses Kerja GAN\n",
    "1.\tTraining Generator:\n",
    "- Generator mengambil input berupa vektor noise acak dan memprosesnya melalui beberapa lapisan neural network untuk menghasilkan data yang mirip dengan data pelatihan.\n",
    "- Output dari Generator adalah gambar palsu (fake image) yang mencoba meniru data asli.\n",
    "2.\tTraining Discriminator:\n",
    "- Discriminator diberi dua set data: satu set data asli dari data pelatihan, dan satu set data palsu yang dihasilkan oleh Generator.\n",
    "- Discriminator bertugas untuk mengklasifikasikan setiap gambar sebagai asli atau palsu.\n",
    "- Discriminator dilatih untuk memaksimalkan akurasi dalam membedakan antara data asli dan data palsu.\n",
    "3.\tAdversarial Training:\n",
    "- Proses pelatihan GAN adalah permainan zero-sum antara Generator dan Discriminator. Generator mencoba untuk menipu Discriminator dengan menghasilkan gambar yang semakin mirip dengan data asli, sementara Discriminator berusaha untuk meningkatkan kemampuannya dalam membedakan data asli dan palsu.\n",
    "- Generator dilatih untuk meminimalkan kemampuan Discriminator dalam mendeteksi bahwa gambar yang dihasilkan adalah palsu.\n",
    "- Discriminator dilatih untuk memaksimalkan akurasi dalam membedakan gambar asli dan palsu.\n",
    "Langkah-langkah Pelatihan\n",
    "1.\tLangkah 1: Update Discriminator\n",
    "- Ambil batch data asli dari data pelatihan dan labelnya sebagai \"real\".\n",
    "- Ambil batch data palsu yang dihasilkan oleh Generator dan labelnya sebagai \"fake\".\n",
    "- Latih Discriminator dengan data asli dan palsu untuk membedakan keduanya dengan benar.\n",
    "2.\tLangkah 2: Update Generator\n",
    "- Ambil batch vektor noise acak dan hasilkan data palsu menggunakan Generator.\n",
    "- Labelkan data palsu ini sebagai \"real\" untuk melatih Generator.\n",
    "- Latih Generator untuk meminimalkan kemampuan Discriminator dalam mendeteksi bahwa data palsu adalah palsu.\n",
    "Tujuan Akhir\n",
    "Tujuan akhir dari pelatihan GAN adalah mencapai suatu titik di mana Generator menghasilkan data yang sangat mirip dengan data asli sehingga Discriminator tidak dapat membedakan dengan baik antara data asli dan data palsu. Pada titik ini, Generator dianggap telah berhasil mempelajari distribusi data asli dan dapat menghasilkan data yang realistis.\n",
    "Kesimpulan\n",
    "GAN menggunakan pendekatan adversarial dengan dua model yang saling berkompetisi untuk menghasilkan data baru yang menyerupai data asli. Generator berusaha untuk menghasilkan data palsu yang realistis, sementara Discriminator berusaha untuk membedakan antara data asli dan palsu. Proses pelatihan yang berkelanjutan ini membantu GAN mencapai hasil yang sangat realistis dalam berbagai aplikasi, seperti generasi gambar, peningkatan resolusi gambar, dan banyak lagi.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def build_generator(noise_dim, num_classes, img_size):\n",
    "    noise_input = layers.Input(shape=(noise_dim,))\n",
    "    label_input = layers.Input(shape=(num_classes,))\n",
    "    \n",
    "    x = layers.Concatenate()([noise_input, label_input])\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = layers.Dense(256, activation='relu')(x)\n",
    "    x = layers.Dense(512, activation='relu')(x)\n",
    "    x = layers.Dense(1024, activation='relu')(x)\n",
    "    x = layers.Dense(img_size * img_size, activation='tanh')(x)  # Use 'tanh' activation for the output layer\n",
    "    \n",
    "    img_output = layers.Reshape((img_size, img_size, 1))(x)\n",
    "    \n",
    "    model = tf.keras.models.Model([noise_input, label_input], img_output)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator(img_size, num_classes):\n",
    "    img_input = layers.Input(shape=(img_size, img_size, 1))\n",
    "    label_input = layers.Input(shape=(num_classes,))\n",
    "    \n",
    "    label_embedding = layers.Dense(img_size * img_size)(label_input)\n",
    "    label_embedding = layers.Reshape((img_size, img_size, 1))(label_embedding)\n",
    "    \n",
    "    x = layers.Concatenate()([img_input, label_embedding])\n",
    "    \n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(512, activation='relu')(x)\n",
    "    x = layers.Dense(1024, activation='relu')(x)\n",
    "    x = layers.Dense(512, activation='relu')(x)\n",
    "    x = layers.Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    model = tf.keras.models.Model([img_input, label_input], x)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train the GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:71: UserWarning: The model does not have any trainable weights.\n",
      "  warnings.warn(\"The model does not have any trainable weights.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/50, D loss: 0.8981834650039673, G loss: [array(0.8197177, dtype=float32), array(0.8197177, dtype=float32), array(0.4921875, dtype=float32)]\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_train_function.<locals>.one_step_on_iterator at 0x0000022830A46020> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_train_function.<locals>.one_step_on_iterator at 0x0000022830ADAFC0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "Epoch 10/50, D loss: 2.183459758758545, G loss: [array(2.2586024, dtype=float32), array(2.2586024, dtype=float32), array(0.04758523, dtype=float32)]\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "Epoch 20/50, D loss: 2.7320003509521484, G loss: [array(2.7685497, dtype=float32), array(2.7685497, dtype=float32), array(0.02715774, dtype=float32)]\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "Epoch 30/50, D loss: 2.9486851692199707, G loss: [array(2.972238, dtype=float32), array(2.972238, dtype=float32), array(0.01965726, dtype=float32)]\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Epoch 40/50, D loss: 3.0652079582214355, G loss: [array(3.082372, dtype=float32), array(3.082372, dtype=float32), array(0.01657774, dtype=float32)]\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Hyperparameters\n",
    "noise_dim = 100\n",
    "num_classes = 10\n",
    "img_size = 28\n",
    "batch_size = 64\n",
    "epochs = 50  # Reduced number of epochs\n",
    "\n",
    "# Load and preprocess data\n",
    "(x_train, y_train), (_, _) = mnist.load_data()\n",
    "x_train = (x_train.astype(np.float32) - 127.5) / 127.5\n",
    "x_train = np.expand_dims(x_train, axis=-1)\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "\n",
    "# Build and compile the discriminator\n",
    "discriminator = build_discriminator(img_size, num_classes)\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "\n",
    "# Build the generator\n",
    "generator = build_generator(noise_dim, num_classes, img_size)\n",
    "\n",
    "# Create GAN model\n",
    "noise_input = layers.Input(shape=(noise_dim,))\n",
    "label_input = layers.Input(shape=(num_classes,))\n",
    "img_output = generator([noise_input, label_input])\n",
    "\n",
    "discriminator.trainable = False\n",
    "validity = discriminator([img_output, label_input])\n",
    "\n",
    "gan = tf.keras.models.Model([noise_input, label_input], validity)\n",
    "gan.compile(loss='binary_crossentropy', optimizer=Adam())\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    # Train discriminator\n",
    "    idx = np.random.randint(0, x_train.shape[0], batch_size)\n",
    "    imgs, labels = x_train[idx], y_train[idx]\n",
    "\n",
    "    noise = np.random.normal(0, 1, (batch_size, noise_dim))\n",
    "    gen_imgs = generator.predict([noise, labels])\n",
    "\n",
    "    real = np.ones((batch_size, 1))\n",
    "    fake = np.zeros((batch_size, 1))\n",
    "\n",
    "    d_loss_real = discriminator.train_on_batch([imgs, labels], real)\n",
    "    d_loss_fake = discriminator.train_on_batch([gen_imgs, labels], fake)\n",
    "    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "    # Train generator\n",
    "    noise = np.random.normal(0, 1, (batch_size, noise_dim))\n",
    "    valid_y = np.ones((batch_size, 1))\n",
    "\n",
    "    g_loss = gan.train_on_batch([noise, labels], valid_y)\n",
    "\n",
    "    # Print the progress\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}/{epochs}, D loss: {d_loss[0]}, G loss: {g_loss}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FID Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m87910968/87910968\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 1us/step\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 2s/step\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 1s/step\n",
      "FID Score: 111.08509142725205\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "from scipy.linalg import sqrtm\n",
    "\n",
    "def calculate_fid(generator, real_images, noise_dim, num_classes, num_images=1000):\n",
    "    # Load InceptionV3 model\n",
    "    model = InceptionV3(include_top=False, pooling='avg', input_shape=(299, 299, 3))\n",
    "\n",
    "    # Generate fake images\n",
    "    noise = np.random.normal(0, 1, (num_images, noise_dim))\n",
    "    labels = np.random.randint(0, num_classes, num_images)\n",
    "    labels = to_categorical(labels, num_classes)\n",
    "    fake_images = generator.predict([noise, labels])\n",
    "    fake_images = np.repeat(fake_images, 3, axis=-1)  # Convert to 3 channels\n",
    "    fake_images = preprocess_input(tf.image.resize(fake_images, (299, 299)))\n",
    "\n",
    "    # Resize real images to match InceptionV3 input size\n",
    "    real_images = np.repeat(real_images, 3, axis=-1)  # Convert to 3 channels\n",
    "    real_images_resized = preprocess_input(tf.image.resize(real_images, (299, 299)))\n",
    "\n",
    "    # Calculate activations\n",
    "    act1 = model.predict(real_images_resized)\n",
    "    act2 = model.predict(fake_images)\n",
    "\n",
    "    # Calculate mean and covariance\n",
    "    mu1, sigma1 = act1.mean(axis=0), np.cov(act1, rowvar=False)\n",
    "    mu2, sigma2 = act2.mean(axis=0), np.cov(act2, rowvar=False)\n",
    "\n",
    "    # Calculate FID\n",
    "    ssdiff = np.sum((mu1 - mu2)**2.0)\n",
    "    covmean = sqrtm(sigma1.dot(sigma2))\n",
    "    if np.iscomplexobj(covmean):\n",
    "        covmean = covmean.real\n",
    "\n",
    "    fid = ssdiff + np.trace(sigma1 + sigma2 - 2.0 * covmean)\n",
    "    return fid\n",
    "\n",
    "# Calculate FID score\n",
    "fid_score = calculate_fid(generator, x_train[:1000], noise_dim, num_classes)\n",
    "print(f\"FID Score: {fid_score}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
